{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnrqKzOIOZKh"
   },
   "source": [
    "# Contribution\n",
    "\n",
    "SEAN TAN KAI WEN (U2240611G) - Trained and tested model\n",
    "\n",
    "CHAN MIN ADELINE ALYSSA (U2221138E) - Trained and tested model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRX5if0ibbek"
   },
   "source": [
    "# Project 1: Teaching NanoGPT to Do Math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R904PRwlbbSh"
   },
   "source": [
    "Our project attempts to use reinforcement learning DPO to teach NanoGPT to solve some simple algebra and arithmetic problems.\n",
    "\n",
    "References used:\n",
    "- https://huggingface.co/blog/pref-tuning\n",
    "- https://github.com/togethercomputer/together-cookbook/blob/main/Finetuning/DPO_Finetuning.ipynb\n",
    "- https://docs.pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0BepXb4ZoBk"
   },
   "source": [
    "### Step 1: Install necesscary packages\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36235,
     "status": "ok",
     "timestamp": 1761287862840,
     "user": {
      "displayName": "Sean Tan",
      "userId": "15677748161343396862"
     },
     "user_tz": -480
    },
    "id": "lpOd0T6kOW3I",
    "outputId": "bd88b510-0608-4301-da17-27a1ae037c6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipykernel in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (6.30.1)\n",
      "Requirement already satisfied: matplotlib in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (3.9.4)\n",
      "Requirement already satisfied: torch in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (2.8.0)\n",
      "Requirement already satisfied: numpy in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (2.0.2)\n",
      "Requirement already satisfied: transformers in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (4.57.0)\n",
      "Requirement already satisfied: datasets in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (3.6.0)\n",
      "Requirement already satisfied: tiktoken in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (0.12.0)\n",
      "Requirement already satisfied: wandb in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (0.22.2)\n",
      "Requirement already satisfied: tqdm in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (4.67.1)\n",
      "Requirement already satisfied: appnope>=0.1.2 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from ipykernel) (0.1.4)\n",
      "Requirement already satisfied: comm>=0.1.1 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from ipykernel) (0.2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from ipykernel) (1.8.16)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from ipykernel) (8.18.1)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from ipykernel) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from ipykernel) (5.8.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from ipykernel) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: packaging>=22 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from ipykernel) (25.0)\n",
      "Requirement already satisfied: psutil>=5.7 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from ipykernel) (5.8.0)\n",
      "Requirement already satisfied: pyzmq>=25 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from ipykernel) (27.1.0)\n",
      "Requirement already satisfied: tornado>=6.2 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from ipykernel) (6.5.1)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from ipykernel) (5.14.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from matplotlib) (6.5.2)\n",
      "Requirement already satisfied: filelock in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: xxhash in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: click>=8.0.1 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: eval-type-backport in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from wandb) (0.2.2)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from wandb) (3.1.45)\n",
      "Requirement already satisfied: platformdirs in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from wandb) (4.3.8)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from wandb) (6.32.1)\n",
      "Requirement already satisfied: pydantic<3 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from wandb) (2.11.10)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from wandb) (2.42.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from pydantic<3->wandb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from pydantic<3->wandb) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from requests->transformers) (2025.10.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.21.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.23.0)\n",
      "Requirement already satisfied: decorator in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (5.2.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (0.19.2)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (2.19.2)\n",
      "Requirement already satisfied: stack-data in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (0.6.3)\n",
      "Requirement already satisfied: exceptiongroup in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (1.3.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\n",
      "Requirement already satisfied: wcwidth in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.3 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from jupyter-client>=8.0.0->ipykernel) (8.7.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /Users/seantan/Desktop/Work/School/Y4S1/SC3000/Sean/Lab1/Code/.conda/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipykernel matplotlib torch numpy transformers datasets tiktoken wandb tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('.')  # so 'model.py' is importable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzJkegxkZr2u"
   },
   "source": [
    "### Step 2: Package imports and configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfFooM8Qcpcj"
   },
   "source": [
    "\n",
    "Above, we start by configuring our hyperparameters. Several key parameters we explored and modified include:\n",
    "\n",
    "|Paramter| Value |\n",
    "|-----------|-----------|\n",
    "| base_lr   | 1e-3  |\n",
    "| epochs   | 20 |\n",
    "| max_new_tokens   | 50   |\n",
    "| temperature   | 1e-30   |\n",
    "| top_k   | None   |\n",
    "| beta   | 0.8   |\n",
    "\n",
    "\n",
    "### base_lr:\n",
    "*   The base learning rate for our AdamW optimizer. Controls how much the model's weights are updated based on computed gradients.\n",
    "*   We varied this value from 1e-6 to 1e-3 until we discovered the optimal base_lr by comparing the output from our testset. If we set the learning rate too small, the model may make very little progress in finding the optimal loss. Likewise, if we set the learning rate too large, the model may overlook the optimal loss.\n",
    "\n",
    "### epochs:\n",
    "*   number of passes done over the dataset. A balance here had to be struck between a high number of epochs leading to overfitting on our training dataset and computational inefficiencies and a low number of epochs leading to underfitting and not converging.\n",
    "\n",
    "\n",
    "### max_new_tokens:\n",
    "*   determines the maximum number of generated tokens. It controls the length of the generated output. Sticking to the default value of 200 results the model producing outputs and that includes unnecessary and meaningless text. Hence max_new_tokens was reduced to 50, which was ideal in this math context.\n",
    "\n",
    "### temperature:\n",
    "*    We discovered that a low temperature is ideal for our model as high temperatures lead to more randomess in the output. Hence, a low temperature serves more deterministic outputs which is ideal for arithemtic equations. Allows greedy decoding. On a technical side, if we chose temperature smaller than 1e-30, it is essentially the same as setting temperature to 0 because it will throw CUDA device assert errors.\n",
    "\n",
    "### top_k:\n",
    "*   determines how many of the top tokens will be considered. Since we're using a near-zero temperature, our model will always choose the token with the highest probability, maintaining the deterministic behavior we expect from it. top_k is set to None because DPO needs unaltered probability distribution and math outputs do not benefit from controlled randomness.\n",
    "\n",
    "### beta:\n",
    "*   is the hyperparamter of the implicit reward of the perferred (positive) response over the negative response. Here we tested several betas from 0.5 to 0.9 and compared the models output on the testing set and identified the optimal beta. Since the optimal beta is set at 0.8, positive samples are weighted 25% more strongly in the DPO algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "F170WTa-PQDV"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import pickle\n",
    "sys.path.append(os.path.abspath('..'))  # add parent dir relative to notebook\n",
    "from model import GPT, GPTConfig\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "# Configuration\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "base_lr = 1e-3\n",
    "epochs = 20\n",
    "beta = 0.8\n",
    "batch_size = 64\n",
    "max_length =64\n",
    "num_samples = 1\n",
    "max_new_tokens = 50\n",
    "temperature = 1e-30\n",
    "top_k = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlpxY3b2qtQU"
   },
   "source": [
    "### Tokenization and Data Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNrPw5RyuBhP"
   },
   "source": [
    "Here, we implemented character tokenization that converts text to integer sequences that the model can process.\n",
    "unk_id is returned to handle any unknown characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "oPse4b1sqs31"
   },
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "with open(\"../sft/meta.pkl\", \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
    "\n",
    "unk_id = stoi.get(\"<unk>\", 0)  # fallback to 0 if no <unk> in vocab\n",
    "\n",
    "def encode(s):\n",
    "    return [stoi.get(c, unk_id) for c in s]\n",
    "# def encode(s): return [stoi[c] for c in s]\n",
    "def decode(l): return ''.join([itos[i] for i in l])\n",
    "\n",
    "EOS_ID = stoi.get(\"\\n\", None)  # we'll append this at train time and stop on it at test time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1761287868800,
     "user": {
      "displayName": "Sean Tan",
      "userId": "15677748161343396862"
     },
     "user_tz": -480
    },
    "id": "hQcU-kJuEHYZ",
    "outputId": "553133df-482f-4f83-f881-a02eeb38b5b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.8.0 cuda: None\n",
      "cuda available: False\n",
      "device count: 0\n"
     ]
    }
   ],
   "source": [
    "import torch, platform\n",
    "print(\"torch:\", torch.__version__, \"cuda:\", torch.version.cuda)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"device count:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IsPy2E0_Fu9Q"
   },
   "outputs": [],
   "source": [
    "meta['stoi']['!'] = 0\n",
    "meta['itos'][0] = '!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29TEJp8rvgKM"
   },
   "source": [
    "stoi is string to integer mapping, while itos is integer to string mapping.\n",
    "\n",
    "If there is an unknown character faced, or special symbols such as !, they will be encoded to 0.\n",
    "\n",
    "If there is a line spacing \"\\n\", the encoder returns None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HftCrYwZx0L"
   },
   "source": [
    "### Step 3: Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ouajoulnFu6-"
   },
   "outputs": [],
   "source": [
    "def compute_logprob(input_ids):\n",
    "    inputs = input_ids[:, :-1]\n",
    "    targets = input_ids[:, 1:]\n",
    "    logits, _ = gpt(inputs, full_seq=True)\n",
    "    B, T, V = logits.size()\n",
    "    logits_flat = logits.reshape(-1, V)\n",
    "    targets_flat = targets.reshape(-1)\n",
    "    loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=0, reduction='none')\n",
    "    loss = loss.reshape(B, T)\n",
    "    attention_mask = (targets != 0).float()\n",
    "    loss = (loss * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n",
    "    return -loss\n",
    "\n",
    "def pad_or_truncate(seq, max_length):\n",
    "    return seq[-max_length:] if len(seq) > max_length else seq + [0] * (max_length - len(seq))\n",
    "\n",
    "def get_batches(lines, batch_size):\n",
    "    random.shuffle(lines)\n",
    "    #for l in lines:\n",
    "    #    print(l[1])\n",
    "    for i in range(0, len(lines), batch_size):\n",
    "        batch = lines[i:i+batch_size]\n",
    "        if len(batch) < batch_size:\n",
    "            continue\n",
    "        neg_inputs = [pad_or_truncate(encode(p['negative'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
    "        pos_inputs = [pad_or_truncate(encode(p['positive'] + '\\n\\n\\n\\n'), max_length) for p in batch]\n",
    "        neg_tensor = torch.tensor(neg_inputs, dtype=torch.long, device=device)\n",
    "        pos_tensor = torch.tensor(pos_inputs, dtype=torch.long, device=device)\n",
    "        yield neg_tensor, pos_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLSi7qqBZ7G-"
   },
   "source": [
    "### Step 4: Load the pretrained NanoGPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8317,
     "status": "ok",
     "timestamp": 1761287877130,
     "user": {
      "displayName": "Sean Tan",
      "userId": "15677748161343396862"
     },
     "user_tz": -480
    },
    "id": "-X4oW8-tFu4u",
    "outputId": "f0348269-2c8b-40b6-a97b-5e81aa5af156"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(74, 348)\n",
       "    (wpe): Embedding(256, 348)\n",
       "    (drop): Dropout(p=0.2, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=348, out_features=1044, bias=False)\n",
       "          (c_proj): Linear(in_features=348, out_features=348, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=348, out_features=1392, bias=False)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=1392, out_features=348, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=348, out_features=74, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load(\"../sft/gpt.pt\", map_location=device)\n",
    "gptconf = GPTConfig(**ckpt['model_args'])\n",
    "gpt = GPT(gptconf)\n",
    "state_dict = ckpt['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k in list(state_dict.keys()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "gpt.load_state_dict(state_dict)\n",
    "gpt.to(device).train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FI6QyGmuZ9it"
   },
   "source": [
    "### Step 5: Load Data\n",
    "\n",
    "We generated this training json dataset using a generator python script (pos_neg_pairs_idk_less_extreme_FIXED.py) that outputs a .json. It consists of 100k items with each items being of either:\n",
    "\n",
    "            * \"x_times_b_eq_c\",\n",
    "            * \"x_plus_b_eq_c\",\n",
    "            * \"x_minus_b_eq_c\",\n",
    "            * \"b_minus_x_eq_c\",\n",
    "            * \"b_over_x_eq_c\",\n",
    "            * \"a_plus_b_q\",\n",
    "            * \"a_minus_b_q\",\n",
    "            * \"a_times_b_q\",\n",
    "            * \"a_over_b_q\",\n",
    "\n",
    "\n",
    "Additionally, to ensure the model is trained to recognize that the position of operands may vary while the underlying arithmetic relationship remains the same, we included sequential variations. For example:\n",
    "\n",
    "x + 4 = 10\n",
    "\n",
    "4 + x = 10\n",
    "\n",
    "While the dataset only contains 100k entries, careful thought and experimentation was carried out in order to balance potential overfitting and wider generalization.\n",
    "\n",
    "* A dataset with a narrower numerical range could result in overfitting to these specific values but failing to generalize beyond unseen values.\n",
    "* A dataset with a wider numerical range may lead to many extreme cases which the model might struggle to learn pattens from. An example would be 8/11 = 0.727272..., or 999*999, which are not covered in the test set.\n",
    "* Hence a tradeoff must be found between coverage and diversity\n",
    "\n",
    "The key component used behind the generation of data was the random number generator, where it will randomly select a expression of a specific operand. After that, the random number generator is used again to randomly choose a number within a specified range, to be used in the expression.\n",
    "\n",
    "We realized that using the random number generator to randomly choose the expressions leads to a skewed distribution of expressions, which led to suboptimal test results. As such, we decided to have a equal proportion of expressions to further improve the test results.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1235,
     "status": "ok",
     "timestamp": 1761287878363,
     "user": {
      "displayName": "Sean Tan",
      "userId": "15677748161343396862"
     },
     "user_tz": -480
    },
    "id": "ZjBG9FLTFu2Z",
    "outputId": "a7a6a241-a866-439d-9cdb-bb22ca8eda6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 100000\n",
      "{'positive': '3-2=? The answer is 1 because 3-2 equals 1.', 'negative': '3-2=? Sorry, I do not know!', 'Q': '3-2=?', 'A': '1', 'type': 'a_minus_b_q'}\n",
      "{'positive': '522/29=? The answer is 18 because 522/29 equals 18.', 'negative': '522/29=? Sorry, I do not know!', 'Q': '522/29=?', 'A': '18', 'type': 'a_over_b_q'}\n",
      "{'positive': '86-69=? The answer is 17 because 86-69 equals 17.', 'negative': '86-69=? Sorry, I do not know!', 'Q': '86-69=?', 'A': '17', 'type': 'a_minus_b_q'}\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "with open(\"pos_neg_pairs_idk_less_extreme_FIXED.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# check length\n",
    "print(\"Number of samples:\", len(data))\n",
    "\n",
    "# inspect a few entries\n",
    "for i in range(3):\n",
    "    print(data[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FkbnB0b1z4Z"
   },
   "source": [
    "In DPO, the model is taught by showing it the positive and negative responses to the same question:\n",
    "\n",
    "* Positive Response: The correct, preferred response - 'x+95=98, x=? The answer is 3 because 98-95 equals 3.'\n",
    "* Negative Response: An incorrect, undesirable response - 'x+95=98, x=? Sorry, I do not know!'\n",
    "\n",
    "The model learns to increase probability of positive samples and decrease probability of negative samples. These contrastive examples are used to ensure the model attempts to computes the answer as we intend the model to learn mathematical reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "865ZFKrOaEI1"
   },
   "source": [
    "### Step 6: Build the optimizer and scheduler\n",
    "\n",
    "We use an AdamW optimizer with a warmup + cosine annealing learning rate schedule to ensure stable and efficient training. AdamW is a variant of the Adam optimizer; however, AdamW decouples weight decay from the gradiant update leading to more consistent regularization and better generalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zk7AASwraErQ"
   },
   "outputs": [],
   "source": [
    "# --- OPTIMIZER ---\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=base_lr, betas=(0.9, 0.95), eps=1e-8, weight_decay=0.1)\n",
    "\n",
    "# --- SCHEDULER ---\n",
    "import math\n",
    "steps_per_epoch = max(1, len(data) // batch_size)  # clamp to at least 1\n",
    "total_steps = epochs * steps_per_epoch\n",
    "warmup_steps = max(1, int(0.4 * total_steps)) # CHANGED HERE\n",
    "min_lr = base_lr * 0.05 # CHANGED HERE\n",
    "\n",
    "def lr_lambda(step):\n",
    "    if step < warmup_steps: # Warmup Phase\n",
    "        return step / warmup_steps\n",
    "    progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
    "    cosine = 0.5 * (1.0 + math.cos(math.pi * progress)) # Cosine Delay Phase\n",
    "    return (min_lr / base_lr) + (1.0 - (min_lr / base_lr)) * cosine\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OBK8uTx9IEZ"
   },
   "source": [
    "|Paramter| Value |\n",
    "|-----------|-----------|\n",
    "| warmup_ratio   | 0.4  |\n",
    "| min_lr_ratio   | 0.05 |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lr_lambda defines the model's learning rate schedule\n",
    "\n",
    "\n",
    "Warmup ratio: indicates a linear increase of learning rate from 0 to base_lr over warmup_steps. In this case 40% of total training steps is spent ramping up the learning rate from 0 to base_lr\n",
    "\n",
    "Min_lr_ratio: the ratio between minimum learning rate and base learning rate during cosine or linear decay, used in calculating the minimum learning rate.\n",
    "\n",
    "Cosine decay phase: gradually decreases learning rate from base_lr to min_lr using a half cosine curve.\n",
    "\n",
    "This ensures a smooth transition and prevents sudden jumps in learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JC_Eje8NaJAD"
   },
   "source": [
    "### Step 7: Begin training\n",
    "\n",
    "Here we begin our training loop for the model using a DPO algorithm to train the model on positive and negative sequences.\n",
    "\n",
    "For each epoch loop, we then loop over each batch in said epoch.\n",
    "The negative and positive examples passed into compute_logprob and the respective log proability of the sequence is returned for both.\n",
    "\n",
    "neg_logprob and pos_logprob is then passed to the loss function:\n",
    "\n",
    "        loss = -F.logsigmoid((pos_logprob - neg_logprob) / beta).mean() - pos_logprob.mean() * 0.1\n",
    "\n",
    "pos_logprob - neg_logprob measures how much more likely the model thinks the positive is compared to the negative example. logsigmoid then converts this to a smooth and differentiable loss.\n",
    "\n",
    "      - pos_logprob.mean() * 0.1\n",
    "is a KL-Divergence Regularization penalty to encourages positive sequences to remain highly probable\n",
    "\n",
    "The beta parameter controls the weight of this preference\n",
    "\n",
    "Our loss function directly encourages the model to assign higher probailities to correct answers rather than incorrect ones.\n",
    "\n",
    "We then start the backpropagation process and compute the gradient loss over all the parameters. Gradiants are then rescaled to not exceed 1.0 to present exploding gradiants that can destabilize the model.\n",
    "\n",
    "The AdamW optimizer then update the model's parameters and the learning rate is adjusted to the cosine delay with warmup. We then clear the optimization for the next batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1392789,
     "status": "ok",
     "timestamp": 1761276120356,
     "user": {
      "displayName": "Sean Tan",
      "userId": "15677748161343396862"
     },
     "user_tz": -480
    },
    "id": "tszWZtWnFu0K",
    "outputId": "9f48d451-ca45-40f6-a838-882a16c69d6e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1562it [01:07, 22.99it/s, loss=0.0284, lr=0.000125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo/dpo_epoch1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1562it [01:06, 23.46it/s, loss=0.024, lr=0.00025]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo/dpo_epoch2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1562it [01:06, 23.32it/s, loss=0.0224, lr=0.000375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo/dpo_epoch3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1562it [01:06, 23.49it/s, loss=0.0198, lr=0.0005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo/dpo_epoch4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1562it [01:06, 23.64it/s, loss=0.0198, lr=0.000625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo/dpo_epoch5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1562it [01:06, 23.59it/s, loss=0.0191, lr=0.00075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo/dpo_epoch6.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1562it [01:06, 23.61it/s, loss=0.0195, lr=0.000875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo/dpo_epoch7.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1562it [01:06, 23.56it/s, loss=0.0193, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo/dpo_epoch8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1562it [01:06, 23.63it/s, loss=0.0182, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo/dpo_epoch9.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1562it [01:06, 23.59it/s, loss=0.0189, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo/dpo_epoch10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1562it [01:06, 23.62it/s, loss=0.0177, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo/dpo_epoch11.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1562it [01:06, 23.42it/s, loss=0.0173, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo/dpo_epoch12.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1562it [01:06, 23.34it/s, loss=0.0183, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo/dpo_epoch13.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1562it [01:06, 23.52it/s, loss=0.0183, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo/dpo_epoch14.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1562it [01:06, 23.60it/s, loss=0.0185, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo/dpo_epoch15.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1562it [01:06, 23.49it/s, loss=0.0178, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo/dpo_epoch16.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1562it [01:06, 23.53it/s, loss=0.0185, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo/dpo_epoch17.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1562it [01:06, 23.46it/s, loss=0.0181, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo/dpo_epoch18.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1562it [01:06, 23.50it/s, loss=0.0173, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo/dpo_epoch19.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1562it [01:06, 23.57it/s, loss=0.0174, lr=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to ./dpo/dpo_epoch20.pt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "total_steps = len(data) // batch_size\n",
    "global_step = 0\n",
    "for epoch in range(epochs):\n",
    "    pbar = tqdm(get_batches(data, batch_size))\n",
    "    for step, (neg_tensor,pos_tensor) in enumerate(pbar):\n",
    "        ###########################################################\n",
    "        # Please complete the training code here!\n",
    "        # Examples:\n",
    "        # ...\n",
    "        neg_logprob = compute_logprob(neg_tensor)\n",
    "        pos_logprob = compute_logprob(pos_tensor)\n",
    "        loss = -F.logsigmoid((pos_logprob - neg_logprob) / beta).mean() - pos_logprob.mean() * 0.1\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(gpt.parameters(), 1.0) #prevent exploding gradients\n",
    "        optimizer.step() #optimizer update\n",
    "        scheduler.step() #learning rate update\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix(loss=float(loss.item()),\n",
    "                         lr=float(optimizer.param_groups[0][\"lr\"]))\n",
    "        # ...\n",
    "        ###########################################################\n",
    "    pbar.close()\n",
    "    ckpt_path = f\"dpo_epoch{epoch+1}.pt\"\n",
    "    torch.save({\n",
    "        \"model_state_dict\": gpt.state_dict(),\n",
    "        \"model_args\": ckpt['model_args'],\n",
    "    }, ckpt_path)\n",
    "    print(f\"Saved checkpoint to {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1XGBNkszE9kR"
   },
   "source": [
    "The model is then saved to memory, creating a checkpoint, enabling the testing process to be more efficient in the future. The checkpoint also allows us to compare the model at various epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqZzmusraLjO"
   },
   "source": [
    "### Step 8: Begin testing\n",
    "\n",
    "The pre-trained model checkpoint is loaded and then run inference (test) mode to generate answers for several test cases.\n",
    "\n",
    "Each prompt is tokenized and is fed into generate() to output an answer. These generated token are decoded back to text and regex is used to extract the final numerical output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1956,
     "status": "ok",
     "timestamp": 1761276322461,
     "user": {
      "displayName": "Sean Tan",
      "userId": "15677748161343396862"
     },
     "user_tz": -480
    },
    "id": "6S3MAp94Fux0",
    "outputId": "daeba811-7649-4cc0-f4ae-0ca92f77391a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 17+19=?\n",
      "A: 36\n",
      "\n",
      "Q: 3*17=?\n",
      "A: 51\n",
      "\n",
      "Q: 72/4=?\n",
      "A: 18\n",
      "\n",
      "Q: 72-x=34,x=?\n",
      "A: 38\n",
      "\n",
      "Q: x*11=44,x=?\n",
      "A: 4\n",
      "\n",
      "Q: 3*17=?\n",
      "A: 51\n",
      "\n",
      "Q: 72/4=?\n",
      "A: 18\n",
      "\n",
      "Q: 72-x=34,x=?\n",
      "A: 38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "\n",
    "# Load the fine-tuned model\n",
    "ckpt_path = \"dpo_epoch20.pt\"\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "gpt = GPT(gptconf).cuda()\n",
    "try:\n",
    "    state_dict = checkpoint['model']\n",
    "except:\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "gpt.load_state_dict(state_dict)\n",
    "# Test\n",
    "gpt.eval()\n",
    "test_set = [\"17+19=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\", \"x*11=44,x=?\", \"3*17=?\", \"72/4=?\", \"72-x=34,x=?\"]\n",
    "with torch.no_grad():\n",
    "    for prompt in test_set:\n",
    "        prompt_ids = encode(prompt)\n",
    "        ###########################################################\n",
    "        # Please complete the test code here!\n",
    "        # ...\n",
    "        x = torch.tensor(prompt_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        out = gpt.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "        if isinstance(out, tuple):\n",
    "          y = out[0]           # token ids\n",
    "          # logits = out[1]    # optional, if you need them\n",
    "        else:\n",
    "          y = out\n",
    "\n",
    "        gen_tokens = y[0, x.size(1):].tolist()   # newly generated tokens\n",
    "        text = decode(gen_tokens)\n",
    "        ans_line = text.split(\"\\n\", 1)[0].strip()\n",
    "        nums = re.findall(r\"-?\\d+\", ans_line)\n",
    "        ans = nums[0] if nums else ans_line\n",
    "        print(f\"Q: {prompt}\\nA: {ans}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xolOeyF4JpXw"
   },
   "source": [
    "To further test the correctness of our model, we then tested it on a larger test set of 1000 samples. This dataset was generated using the same generator for the training dataset to ensure it comes from the same data distribution. Hence, we are able to fairly test it's generalizability and accuracy. We also break down its error by equation type to identify dataset skew. If the model consistently fails on certain types of problems, it may indicate that those types are underrepresented in the training data. Detecting such skews allows us to rebalance the dataset and improving overall generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 706,
     "status": "ok",
     "timestamp": 1761287883865,
     "user": {
      "displayName": "Sean Tan",
      "userId": "15677748161343396862"
     },
     "user_tz": -480
    },
    "id": "g8DlSXjpFAq6",
    "outputId": "e76e49ca-0c3b-47d0-882e-077a2a3d798f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'positive': '36+18=? The answer is 54 because 36+18 equals 54.', 'negative': '36+18=? Sorry, I do not know!', 'Q': '36+18=?', 'A': '54', 'type': 'a_plus_b_q'}\n",
      "{'Q': '36+18=?', 'A': '54', 'type': 'a_plus_b_q'}\n",
      "{'positive': '89-x=42,x=? The answer is 47 because 89-42 equals 47.', 'negative': '89-x=42,x=? Sorry, I do not know!', 'Q': '89-x=42,x=?', 'A': '47', 'type': 'b_minus_x_eq_c'}\n",
      "{'Q': '89-x=42,x=?', 'A': '47', 'type': 'b_minus_x_eq_c'}\n",
      "{'positive': '58-38=? The answer is 20 because 58-38 equals 20.', 'negative': '58-38=? Sorry, I do not know!', 'Q': '58-38=?', 'A': '20', 'type': 'a_minus_b_q'}\n",
      "{'Q': '58-38=?', 'A': '20', 'type': 'a_minus_b_q'}\n",
      "Loaded 1000 test cases.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "with open(\"pos_neg_pairs_idk_less_extreme_FIXED_test.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "\n",
    "test_data = []\n",
    "for ex in raw_data:\n",
    "    q = ex[\"positive\"].split(\"The answer is\")[0].strip()\n",
    "    m = re.search(r\"The answer is\\s+(-?\\d+)\", ex[\"positive\"])\n",
    "    a = m.group(1) if m else None\n",
    "    test_data.append({\"Q\": q, \"A\": a, \"type\": ex['type']})\n",
    "\n",
    "for i in range(3):\n",
    "  print(raw_data[i])\n",
    "  print(test_data[i])\n",
    "\n",
    "print(f'Loaded {len(test_data)} test cases.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 193351,
     "status": "ok",
     "timestamp": 1761288077223,
     "user": {
      "displayName": "Sean Tan",
      "userId": "15677748161343396862"
     },
     "user_tz": -480
    },
    "id": "tn6UL667yzG3",
    "outputId": "ce6fcfbb-83c6-467c-f992-a18146fd2e5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 test cases done...\n",
      "200 test cases done...\n",
      "300 test cases done...\n",
      "400 test cases done...\n",
      "500 test cases done...\n",
      "600 test cases done...\n",
      "700 test cases done...\n",
      "800 test cases done...\n",
      "900 test cases done...\n",
      "1000 test cases done...\n",
      "Total: 1000, Correct: 978, Incorrect: 22\n",
      "\n",
      "Accuracy: 97.80%\n",
      "\n",
      "\n",
      "=== Breakdown by Equation Type ===\n",
      "\n",
      "a_plus_b_q: Correct=99, Incorrect=0, Percentage =100.000%\n",
      "\n",
      "b_minus_x_eq_c: Correct=114, Incorrect=0, Percentage =100.000%\n",
      "\n",
      "a_minus_b_q: Correct=102, Incorrect=0, Percentage =100.000%\n",
      "\n",
      "x_minus_b_eq_c: Correct=123, Incorrect=0, Percentage =100.000%\n",
      "\n",
      "x_times_b_eq_c: Correct=127, Incorrect=0, Percentage =100.000%\n",
      "\n",
      "a_times_b_q: Correct=107, Incorrect=0, Percentage =100.000%\n",
      "\n",
      "b_over_x_eq_c: Correct=106, Incorrect=12, Percentage =89.831%\n",
      "\n",
      "x_plus_b_eq_c: Correct=103, Incorrect=0, Percentage =100.000%\n",
      "\n",
      "a_over_b_q: Correct=97, Incorrect=10, Percentage =90.654%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# Load the fine-tuned model\n",
    "ckpt_path = \"dpo_epoch20.pt\"\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "#gpt = GPT(gptconf).cuda()\n",
    "gpt = GPT(gptconf)\n",
    "\n",
    "try:\n",
    "    state_dict = checkpoint['model']\n",
    "except:\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "gpt.load_state_dict(state_dict)\n",
    "\n",
    "gpt.to(device)\n",
    "# Test\n",
    "gpt.eval()\n",
    "with torch.no_grad():\n",
    "\n",
    "        total = 0\n",
    "        correct_count = 0\n",
    "        breakdown = defaultdict(lambda: {\"correct\": 0, \"incorrect\": 0})\n",
    "        incorrect_list = []\n",
    "\n",
    "        with open(f\"test_cases_with_answers_final.txt\", \"w\") as f:\n",
    "            for prompt in test_data:\n",
    "              prompt_ids = encode(prompt['Q'])\n",
    "\n",
    "              x = torch.tensor(prompt_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "              out = gpt.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "              if isinstance(out, tuple):\n",
    "                y = out[0]           # token ids\n",
    "              else:\n",
    "                y = out\n",
    "\n",
    "              gen_tokens = y[0, x.size(1):].tolist()\n",
    "              text = decode(gen_tokens)\n",
    "              ans_line = text.split(\"\\n\", 1)[0].strip()\n",
    "              nums = re.findall(r\"-?\\d+\", ans_line)\n",
    "              ans = nums[0] if nums else ans_line\n",
    "\n",
    "              ground_truth = prompt['A']\n",
    "              eq_type = prompt['type']\n",
    "\n",
    "              total += 1\n",
    "\n",
    "              if str(ans) == str(ground_truth):\n",
    "                  correct = \"Correct\"\n",
    "                  correct_count += 1\n",
    "                  breakdown[eq_type][\"correct\"] += 1\n",
    "              else:\n",
    "                  correct = \"Incorrect\"\n",
    "                  breakdown[eq_type][\"incorrect\"] += 1\n",
    "                  incorrect_list.append({\n",
    "                      \"Q\": prompt['Q'],\n",
    "                      \"A\": ans,\n",
    "                      \"Ground Truth\": ground_truth,\n",
    "                      \"Type\": eq_type\n",
    "                    })\n",
    "\n",
    "\n",
    "              f.write(f\"Q: {prompt['Q']}\\n\")\n",
    "              f.write(f\"A: {ans}\\n\")\n",
    "              f.write(f\"Ground Truth: {ground_truth}\\n\")\n",
    "              f.write(f\"Equation Type: {eq_type}\\n\")\n",
    "              f.write(f\"Result: {correct}\\n\")\n",
    "              if total % 100 == 0:\n",
    "                print(f\"{total} test cases done...\")\n",
    "\n",
    "            # summary\n",
    "            f.write(\"\\n=== Summary ===\\n\")\n",
    "            f.write(f\"Total: {total}, Correct: {correct_count}, Incorrect: {total - correct_count}\\n\")\n",
    "            f.write(f\"Accuracy: {correct_count/total:.2%}\\n\\n\")\n",
    "\n",
    "            # breakdown by type\n",
    "            f.write(\"=== Breakdown by Equation Type ===\\n\")\n",
    "            for t, stats in breakdown.items():\n",
    "                f.write(f\"{t}: Correct={stats['correct']}, Incorrect={stats['incorrect']}, Percentage ={stats['correct']/(stats['incorrect']+stats['correct']):.3%}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "\n",
    "            f.write(\"=== Incorrect Equations ===\\n\")\n",
    "            for item in incorrect_list:\n",
    "                f.write(f\"Q: {item['Q']}\\n\")\n",
    "                f.write(f\"A: {item['A']}\\n\")\n",
    "                f.write(f\"Ground Truth: {item['Ground Truth']}\\n\")\n",
    "                f.write(f\"Type: {item['Type']}\\n\")\n",
    "                f.write(\"-\"*40+\"\\n\")\n",
    "\n",
    "            print(f\"Total: {total}, Correct: {correct_count}, Incorrect: {total - correct_count}\\n\")\n",
    "            print(f\"Accuracy: {correct_count/total:.2%}\\n\\n\")\n",
    "            print(\"=== Breakdown by Equation Type ===\\n\")\n",
    "            for t, stats in breakdown.items():\n",
    "                print(f\"{t}: Correct={stats['correct']}, Incorrect={stats['incorrect']}, Percentage ={stats['correct']/(stats['incorrect']+stats['correct']):.3%}\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
